{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Web Berita CNN Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apa itu Crawling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawling merupakan proses search engine untuk menemukan konten atau sesuatu situs halaman yang ada. Dalam bahasa kerennya crawling atau web crawling merupakan proses dimana search engine mengirimkan bot atau robot yang disebut (crawler atau spider) yang digunakan untuk menemukan konten-konten yang ada.\n",
    "\n",
    "Yang dimaksud konten yaitu bervariasi, mulai dari halaman website yang saya lakukan ini, kemudian gambar, video, dokumen, dan lain sebagainya. Seperti halnya laba-laba, datang ke sebuah jaring dan melihat beberapa halaman website, kemudian mengikuti link yang terdapat di halaman website tersebut untuk mencari URL yang baru.\n",
    "\n",
    "Ketika ada pengguna yang mencari sebuah konten di search engine dengan keyword tertentu, search engine akan mencarinya di indeks dan menentukan konten mana yang paling sesuai untuk pengguna tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proses Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool atau Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library yang dibutuhkan\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Request** digunakan untuk mengambil html/http dari sebuah website.\n",
    "* **BeautifulSoup** berfungsi untuk mengambil data dari html/xml.\n",
    "* **Time** berfungsi untuk memberikan jeda ketika ingin berpindah halaman.\n",
    "* **Pandas** digunakan untuk membuat dataframe agar mudah dibaca.\n",
    "* **tqdm** Untuk mentracking proses program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi Clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\ttext = text.replace('\\xa0', '',)\n",
    "\treturn text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi untuk membersihkan text yang tidak diinginkan, atau mengganggu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi scrape_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news(soup):\n",
    "\tberita = {}\n",
    "\ttexts = []\n",
    "\t# TODO:\n",
    "\t# ada struktur aneh https://www.cnnindonesia.com/olahraga/20240830134615-142-1139388/live-report-timnas-indonesia-vs-thailand-u-20\n",
    "\t\n",
    "\tberita[\"judul\"] = soup.title.text\n",
    "\n",
    "\tif 'FOTO:' in berita[\"judul\"]:\n",
    "\t\tdiv_content = soup.find(\"div\", class_=\"detail-text text-cnn_black text-sm grow min-w-0\")\n",
    "\t\tif div_content:\n",
    "\t\t\tfull_text = div_content.get_text(strip=True)\n",
    "\t\t\ttext = full_text.split('--', 1)[-1]\n",
    "\t\t\ttext = text.split('var article')[0].strip()\n",
    "\t\t\t\n",
    "\t\t\tcleaned_text = clean_text(text)\n",
    "\t\t\ttexts.append(cleaned_text)\n",
    "\n",
    "\ttext_list = soup.find(\"div\", class_=\"detail-text text-cnn_black text-sm grow min-w-0\")\n",
    "\tfor text in text_list.find_all(\"p\"):\n",
    "\t\tif 'para_caption' not in text.get('class', []):\n",
    "\t\t\tcleaned_text = clean_text(text.text)\n",
    "\t\t\ttexts.append(cleaned_text)\n",
    "\t\t\n",
    "\tberita[\"isi\"] = \"\\n\".join(texts)\n",
    "\tberita[\"tanggal\"] = soup.find(\"div\", class_=\"container !w-[1100px] overscroll-none\").find_all(\"div\")[1].find_all(\"div\")[4].text\n",
    "\tberita[\"kategori\"] = soup.find(\"a\", attrs={\"aria-label\": \"link description\", \"dtr-act\": \"kanal\"}).text\n",
    "\treturn berita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada fungsi ini berisikan proses pembedahan dan juga pengambilan data pada sebuah website. Mengambil data sesuai struktur HTML/web yang ingin diambil datanya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi get_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "\ttry:\n",
    "\t\tresponse = req.get(url).text\n",
    "\t\treturn bs(response, \"html5lib\")\n",
    "\t\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\t\treturn \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi get_html dengan parameter url digunakan untuk mengambil response atau isi html dari web. Untuk mengambil response tersebut dibutuhkan library request, dan juga BeautifulSoup untuk mendapatkan isi html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi get_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(soup):\n",
    "\tcontainer = soup.find(\"div\", class_=\"container !w-[1100px] overscroll-none\")\n",
    "\tnews_list = container.find_all(\"article\", class_=\"flex-grow\")\n",
    "\treturn news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi get_news berfungsi untuk mengambil semua berita yang ada pada web, yang kemudian didapat kumpulan url berita yang ada pada halaman web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(nasional: int, internasional: int, halaman: int=None):\n",
    "\turl = [\"https://www.cnnindonesia.com/nasional/indeks/3/\", \"https://www.cnnindonesia.com/internasional/indeks/6/\"]\n",
    "\t# url = [\"https://www.cnnindonesia.com/indeks/2/\"]\n",
    "\tnews = []\n",
    "\n",
    "\tnasional_count = 0\n",
    "\tinternasional_count = 0\n",
    "\n",
    "\twhile url:\n",
    "\t\turl_now = url.pop()\n",
    "\t\tcount = 1 # - n\n",
    "\n",
    "\t\tif halaman is None:\n",
    "\t\t\thalaman = int(get_html(url_now).find(\"div\", class_=\"flex gap-5 my-8 items-center justify-center undefined\").find_all(\"a\")[-2].text)\n",
    "\t\t\t# print(f\"Total page: {halaman}\")\n",
    "\n",
    "\t\tfor _ in range(halaman):\n",
    "\t\t\tpage = f\"{url_now}{count}\"   \n",
    "\t\t\tsoup = get_html(page)\n",
    "\t\t\tnews_list = get_news(soup)\n",
    "\t\t\t\n",
    "\t\t\tfor item in tqdm(news_list, desc=f\"Processing page {count}\"):\n",
    "\t\t\t\tnews_url = item.find('a')['href']\n",
    "\t\t\t\t# print(news_url)\n",
    "\t\t\t\tresult = scrape_news(get_html(news_url))\n",
    "\n",
    "\t\t\t\tif nasional_count >= nasional and internasional_count >= internasional:\n",
    "\t\t\t\t\treturn news\n",
    "\n",
    "\t\t\t\tif result['kategori'] == 'Nasional' and nasional_count < nasional:\n",
    "\t\t\t\t\tnews.append(result)\n",
    "\t\t\t\t\tnasional_count += 1\n",
    "\t\t\t\telif result['kategori'] == 'Internasional' and internasional_count < internasional:\n",
    "\t\t\t\t\tnews.append(result)\n",
    "\t\t\t\t\tinternasional_count += 1\n",
    "\t\t\t\t\n",
    "\t\t\tcount+=1\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\n",
    "\t\t\tif (nasional_count >= nasional and 'nasional' in url_now) or (internasional_count >= internasional and 'internasional' in url_now):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\treturn news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menyiapkan link/base url web berita yang ingin dicrawling, terdapat beberapa fungsi yang dipanggil yang sudah dibuat sebelumnya untuk mengambil informasi atau berita pada halaman website. Dalam code tersebut terdapat beberapa tahapan seperti fungsi:\n",
    "* get_html\n",
    "* get_news\n",
    "* scrape_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing page 1: 100%|██████████| 10/10 [00:03<00:00,  3.00it/s]\n",
      "Processing page 2: 100%|██████████| 10/10 [00:03<00:00,  2.68it/s]\n",
      "Processing page 3: 100%|██████████| 10/10 [00:03<00:00,  2.72it/s]\n",
      "Processing page 4: 100%|██████████| 10/10 [00:04<00:00,  2.41it/s]\n",
      "Processing page 5: 100%|██████████| 10/10 [00:03<00:00,  2.54it/s]\n",
      "Processing page 1: 100%|██████████| 10/10 [00:03<00:00,  3.01it/s]\n",
      "Processing page 2: 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n",
      "Processing page 3: 100%|██████████| 10/10 [00:04<00:00,  2.18it/s]\n",
      "Processing page 4: 100%|██████████| 10/10 [00:04<00:00,  2.15it/s]\n",
      "Processing page 5: 100%|██████████| 10/10 [00:04<00:00,  2.43it/s]\n"
     ]
    }
   ],
   "source": [
    "news = main(nasional=50, internasional=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menjalankan program yang sudah dibuat dengan input berapa halaman yang ingin diambil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(news)\n",
    "df\n",
    "df.to_csv(\"data_100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebuah rudal yang ditembakkan dari Yaman jatuh ke area terbuka dan kosong di Israel, tak ada korban jiwa dalam serangan tersebut.\n"
     ]
    }
   ],
   "source": [
    "print(df['isi'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
