{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Web Berita CNN Indonesia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apa itu Crawling?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawling merupakan proses search engine untuk menemukan konten atau sesuatu situs halaman yang ada. Dalam bahasa kerennya crawling atau web crawling merupakan proses dimana search engine mengirimkan bot atau robot yang disebut (crawler atau spider) yang digunakan untuk menemukan konten-konten yang ada.\n",
    "\n",
    "Yang dimaksud konten yaitu bervariasi, mulai dari halaman website yang saya lakukan ini, kemudian gambar, video, dokumen, dan lain sebagainya. Seperti halnya laba-laba, datang ke sebuah jaring dan melihat beberapa halaman website, kemudian mengikuti link yang terdapat di halaman website tersebut untuk mencari URL yang baru.\n",
    "\n",
    "Ketika ada pengguna yang mencari sebuah konten di search engine dengan keyword tertentu, search engine akan mencarinya di indeks dan menentukan konten mana yang paling sesuai untuk pengguna tersebut.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proses Crawling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool atau Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library yang dibutuhkan\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Request** digunakan untuk mengambil html/http dari sebuah website.\n",
    "- **BeautifulSoup** berfungsi untuk mengambil data dari html/xml.\n",
    "- **Time** berfungsi untuk memberikan jeda ketika ingin berpindah halaman.\n",
    "- **Pandas** digunakan untuk membuat dataframe agar mudah dibaca.\n",
    "- **tqdm** Untuk mentracking proses program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi Clean_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\treturn text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi untuk membersihkan text yang tidak diinginkan, atau mengganggu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi scrape_news()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news(soup):\n",
    "\tberita = {}\n",
    "\ttexts = []\n",
    "\t# TODO:\n",
    "\t# ada struktur aneh https://www.cnnindonesia.com/olahraga/20240830134615-142-1139388/live-report-timnas-indonesia-vs-thailand-u-20\n",
    "\t\n",
    "\tberita[\"judul\"] = soup.title.text\n",
    "\n",
    "\tif 'FOTO:' in berita[\"judul\"]:\n",
    "\t\tdiv_content = soup.find(\"div\", class_=\"detail-text text-cnn_black text-sm grow min-w-0\")\n",
    "\t\tif div_content:\n",
    "\t\t\tfull_text = div_content.get_text(strip=True)\n",
    "\t\t\ttext = full_text.split('--', 1)[-1]\n",
    "\t\t\ttext = text.split('var article')[0].strip()\n",
    "\n",
    "\t\t\tcleaned_text = clean_text(text)\n",
    "\t\t\ttexts.append(cleaned_text)\n",
    "\n",
    "\t\tberita[\"tanggal\"] = soup.find(\"div\", class_=\"container !w-[1100px] overscroll-none\").find_all(\"div\")[1].find_all(\"div\")[2].text\n",
    "\n",
    "\telse:\n",
    "\t\ttext_list = soup.find(\"div\", class_=\"detail-text text-cnn_black text-sm grow min-w-0\")\n",
    "\t\tfor text in text_list.find_all(\"p\"):\n",
    "\t\t\tif 'para_caption' not in text.get('class', []):\n",
    "\t\t\t\tcleaned_text = clean_text(text.text)\n",
    "\t\t\t\ttexts.append(cleaned_text)\n",
    "\n",
    "\t\tberita[\"tanggal\"] = soup.find(\"div\", class_=\"container !w-[1100px] overscroll-none\").find_all(\"div\")[1].find_all(\"div\")[3].text\n",
    "\n",
    "\tberita[\"isi\"] = \"\\n\".join(texts)\n",
    "\tberita[\"kategori\"] = soup.find(\"meta\", attrs={'name': 'dtk:namakanal'})['content']\n",
    "\treturn berita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada fungsi ini berisikan proses pembedahan dan juga pengambilan data pada sebuah website. Mengambil data sesuai struktur HTML/web yang ingin diambil datanya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi get_html()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "\ttry:\n",
    "\t\tresponse = req.get(url).text\n",
    "\t\treturn bs(response, \"html5lib\")\n",
    "\t\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\t\treturn \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi get_html dengan parameter url digunakan untuk mengambil response atau isi html dari web. Untuk mengambil response tersebut dibutuhkan library request, dan juga BeautifulSoup untuk mendapatkan isi html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi get_news()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(soup):\n",
    "\tcontainer = soup.find(\"div\", class_=\"container !w-[1100px] overscroll-none\")\n",
    "\tnews_list = container.find_all(\"article\", class_=\"flex-grow\")\n",
    "\treturn news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi get_news berfungsi untuk mengambil semua berita yang ada pada web, yang kemudian didapat kumpulan url berita yang ada pada halaman web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Crawling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(nasional: int, internasional: int):\n",
    "\turl = [\"https://www.cnnindonesia.com/nasional/indeks/3/\", \"https://www.cnnindonesia.com/internasional/indeks/6/\"]\n",
    "\t# url = [\"https://www.cnnindonesia.com/indeks/2/\"]\n",
    "\tnews = []\n",
    "\n",
    "\tnasional_count = 0\n",
    "\tinternasional_count = 0\n",
    "\n",
    "\twhile url:\n",
    "\t\turl_now = url.pop()\n",
    "\t\tcount = 1 # - n\n",
    "\t\thalaman = int(get_html(url_now).find(\"div\", class_=\"flex gap-5 my-8 items-center justify-center undefined\").find_all(\"a\")[-2].text)\n",
    "\t\t\n",
    "\n",
    "\t\tfor _ in range(halaman):\n",
    "\t\t\tpage = f\"{url_now}{count}\" \n",
    "\t\t\tsoup = get_html(page)\n",
    "\t\t\tnews_list = get_news(soup)\n",
    "\t\t\t\n",
    "\t\t\tfor item in tqdm(news_list, desc=f\"Processing page {count}\"):\n",
    "\t\t\t\tnews_url = item.find('a')['href']\n",
    "\t\t\t\t# print(news_url)\n",
    "\t\t\t\tresult = scrape_news(get_html(news_url))\n",
    "\n",
    "\t\t\t\tif nasional_count >= nasional and internasional_count >= internasional:\n",
    "\t\t\t\t\treturn news\n",
    "\n",
    "\t\t\t\tif result['kategori'] == 'nasional' and nasional_count < nasional:\n",
    "\t\t\t\t\tnews.append(result)\n",
    "\t\t\t\t\tnasional_count += 1\n",
    "\n",
    "\t\t\t\telif result['kategori'] == 'internasional' and internasional_count < internasional:\n",
    "\t\t\t\t\tnews.append(result)\n",
    "\t\t\t\t\tinternasional_count += 1\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\tcount+=1\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\t\tif (nasional_count >= nasional and 'nasional' in url_now) or (internasional_count >= internasional and 'internasional' in url_now):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\treturn news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menyiapkan link/base url web berita yang ingin dicrawling, terdapat beberapa fungsi yang dipanggil yang sudah dibuat sebelumnya untuk mengambil informasi atau berita pada halaman website. Dalam code tersebut terdapat beberapa tahapan seperti fungsi:\n",
    "\n",
    "- get_html\n",
    "- get_news\n",
    "- scrape_news\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing page 1: 100%|██████████| 10/10 [00:05<00:00,  1.74it/s]\n",
      "Processing page 1:  10%|█         | 1/10 [00:01<00:12,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "news = main(nasional=1, internasional=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menjalankan program yang sudah dibuat dengan input berapa halaman yang ingin diambil.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>isi</th>\n",
       "      <th>kategori</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOTO: WNI yang Dievakuasi dari Lebanon Telah T...</td>\n",
       "      <td>Senin, 07 Okt 2024 11:50 WIB</td>\n",
       "      <td>Puluhan WNI yang dievakuasi dari Lebanon telah...</td>\n",
       "      <td>internasional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pemotor yang Bonceng Ustaz Maulana Tak Pakai H...</td>\n",
       "      <td>Senin, 07 Okt 2024 11:58 WIB</td>\n",
       "      <td>Seorang pengendara sepeda motor yang memboncen...</td>\n",
       "      <td>nasional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               judul  \\\n",
       "0  FOTO: WNI yang Dievakuasi dari Lebanon Telah T...   \n",
       "1  Pemotor yang Bonceng Ustaz Maulana Tak Pakai H...   \n",
       "\n",
       "                          tanggal  \\\n",
       "0    Senin, 07 Okt 2024 11:50 WIB   \n",
       "1   Senin, 07 Okt 2024 11:58 WIB    \n",
       "\n",
       "                                                 isi       kategori  \n",
       "0  Puluhan WNI yang dievakuasi dari Lebanon telah...  internasional  \n",
       "1  Seorang pengendara sepeda motor yang memboncen...       nasional  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(news)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               judul  \\\n",
      "0  FOTO: WNI yang Dievakuasi dari Lebanon Telah T...   \n",
      "1  Pemotor yang Bonceng Ustaz Maulana Tak Pakai H...   \n",
      "\n",
      "                          tanggal  \\\n",
      "0    Senin, 07 Okt 2024 11:50 WIB   \n",
      "1   Senin, 07 Okt 2024 11:58 WIB    \n",
      "\n",
      "                                                 isi       kategori  \n",
      "0  Puluhan WNI yang dievakuasi dari Lebanon telah...  internasional  \n",
      "1  Seorang pengendara sepeda motor yang memboncen...       nasional  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\LAB\n",
      "[nltk_data]     SISTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\LAB\n",
      "[nltk_data]     SISTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Library untuk data manipulation\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Library untuk text preprocessing\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Library untuk text vectorization/TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\ttext = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', text) # Menghapus https* and www*\n",
    "\ttext = re.sub(r'@[^\\s]+', ' ', text) # Menghapus username\n",
    "\ttext = re.sub(r'[\\s]+', ' ', text) # Menghapus tambahan spasi\n",
    "\ttext = re.sub(r'#([^\\s]+)', ' ', text) # Menghapus hashtags\n",
    "\ttext = re.sub(r'rt', ' ', text) # Menghapus retweet\n",
    "\ttext = text.translate(str.maketrans(\"\",\"\",string.punctuation)) # Menghapus tanda baca\n",
    "\ttext = re.sub(r'\\d', ' ', text) # Menghapus angka\n",
    "\ttext = text.lower()\n",
    "\ttext = text.encode('ascii','ignore').decode('utf-8') #Menghapus ASCII dan unicode\n",
    "\ttext = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "\ttext = text.replace('\\n','') #Menghapus baris baru\n",
    "\ttext = text.strip()\n",
    "\treturn text\n",
    "def stemming_indo(text):\n",
    "\tfactory = StemmerFactory()\n",
    "\tstemmer = factory.create_stemmer()\n",
    "\ttext = ' '.join(stemmer.stem(word) for word in text)\n",
    "\treturn text\n",
    "def clean_stopword(tokens):\n",
    "\tlistStopword =  set(stopwords.words('indonesian'))\n",
    "\tremoved = []\n",
    "\tfor t in tokens:\n",
    "\t\tif t not in listStopword:\n",
    "\t\t\tremoved.append(t)\n",
    "\treturn removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 23.67it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(content):\n",
    "\tresult = []\n",
    "\tfor text in tqdm(content):\n",
    "\t\tcleaned_text = clean_text(text)\n",
    "\t\ttokens = nltk.tokenize.word_tokenize(cleaned_text)\n",
    "\t\tcleaned_stopword = clean_stopword(tokens)\n",
    "\t\tstemmed_text = stemming_indo(cleaned_stopword)\n",
    "\t\tresult.append(stemmed_text)\n",
    "\treturn result\n",
    "\n",
    "df['cleaned_text'] = preprocess_text(df['isi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vsm(data, kategori):\n",
    "\ttfidf = TfidfVectorizer()\n",
    "\ttfidf_matrix = tfidf.fit_transform(data)\n",
    "\tfeature_names = tfidf.get_feature_names_out()\n",
    "\t\n",
    "\tdf_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\tdf_tfidf.insert(0, 'Kategori Berita', kategori.reset_index(drop=True))\n",
    "\n",
    "\treturn tfidf, df_tfidf\n",
    "\n",
    "# tfidf, df_tfidf = tfidf_vsm(df['cleaned_text'], df['kategori'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tugas3/lr_model.pkl', 'rb') as file:  \n",
    "    # Call load method to deserialze \n",
    "    model = pickle.load(file)\n",
    "with open('tugas2/tfidf_model.pkl', 'rb') as file:  \n",
    "    # Call load method to deserialze \n",
    "    tfidf = pickle.load(file)\n",
    "\n",
    "tfidf_matrix = tfidf.transform(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAB SISTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain the logistic regression model with the updated feature matrix\n",
    "model.predict(tfidf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
