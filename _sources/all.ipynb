{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Web Berita CNN Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apa itu Crawling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawling merupakan proses search engine untuk menemukan konten atau sesuatu situs halaman yang ada. Dalam bahasa kerennya crawling atau web crawling merupakan proses dimana search engine mengirimkan bot atau robot yang disebut (crawler atau spider) yang digunakan untuk menemukan konten-konten yang ada.\n",
    "\n",
    "Yang dimaksud konten yaitu bervariasi, mulai dari halaman website yang saya lakukan ini, kemudian gambar, video, dokumen, dan lain sebagainya. Seperti halnya laba-laba, datang ke sebuah jaring dan melihat beberapa halaman website, kemudian mengikuti link yang terdapat di halaman website tersebut untuk mencari URL yang baru.\n",
    "\n",
    "Ketika ada pengguna yang mencari sebuah konten di search engine dengan keyword tertentu, search engine akan mencarinya di indeks dan menentukan konten mana yang paling sesuai untuk pengguna tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proses Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool atau Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library yang dibutuhkan\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Request** digunakan untuk mengambil html/http dari sebuah website.\n",
    "* **BeautifulSoup** berfungsi untuk mengambil data dari html/xml.\n",
    "* **Time** berfungsi untuk memberikan jeda ketika ingin berpindah halaman.\n",
    "* **Pandas** digunakan untuk membuat dataframe agar mudah dibaca.\n",
    "* **tqdm** Untuk mentracking proses program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi Clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\ttext = text.replace('\\xa0', '',)\n",
    "\treturn text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi untuk membersihkan text yang tidak diinginkan, atau mengganggu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi scrape_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news(soup):\n",
    "\tberita = {}\n",
    "\ttexts = []\n",
    "\t# TODO:\n",
    "\t# ada struktur aneh https://www.cnnindonesia.com/olahraga/20240830134615-142-1139388/live-report-timnas-indonesia-vs-thailand-u-20\n",
    "\t\n",
    "\tberita[\"judul\"] = soup.title.text\n",
    "\n",
    "\tif 'FOTO:' in berita[\"judul\"]:\n",
    "\t\tdiv_content = soup.find(\"div\", class_=\"detail-text text-cnn_black text-sm grow min-w-0\")\n",
    "\t\tif div_content:\n",
    "\t\t\tfull_text = div_content.get_text(strip=True)\n",
    "\t\t\ttext = full_text.split('--', 1)[-1]\n",
    "\t\t\ttext = text.split('var article')[0].strip()\n",
    "\t\t\t\n",
    "\t\t\tcleaned_text = clean_text(text)\n",
    "\t\t\ttexts.append(cleaned_text)\n",
    "\n",
    "\ttext_list = soup.find(\"div\", class_=\"detail-text text-cnn_black text-sm grow min-w-0\")\n",
    "\tfor text in text_list.find_all(\"p\"):\n",
    "\t\tif 'para_caption' not in text.get('class', []):\n",
    "\t\t\tcleaned_text = clean_text(text.text)\n",
    "\t\t\ttexts.append(cleaned_text)\n",
    "\t\t\n",
    "\tberita[\"isi\"] = \"\\n\".join(texts)\n",
    "\tberita[\"tanggal\"] = soup.find(\"div\", class_=\"container !w-[1100px] overscroll-none\").find_all(\"div\")[1].find_all(\"div\")[4].text\n",
    "\tberita[\"kategori\"] = soup.find(\"a\", attrs={\"aria-label\": \"link description\", \"dtr-act\": \"kanal\"}).text\n",
    "\treturn berita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada fungsi ini berisikan proses pembedahan dan juga pengambilan data pada sebuah website. Mengambil data sesuai struktur HTML/web yang ingin diambil datanya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi get_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "\ttry:\n",
    "\t\tresponse = req.get(url).text\n",
    "\t\treturn bs(response, \"html5lib\")\n",
    "\t\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\t\treturn \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi get_html dengan parameter url digunakan untuk mengambil response atau isi html dari web. Untuk mengambil response tersebut dibutuhkan library request, dan juga BeautifulSoup untuk mendapatkan isi html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi get_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(soup):\n",
    "\tcontainer = soup.find(\"div\", class_=\"container !w-[1100px] overscroll-none\")\n",
    "\tnews_list = container.find_all(\"article\", class_=\"flex-grow\")\n",
    "\treturn news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi get_news berfungsi untuk mengambil semua berita yang ada pada web, yang kemudian didapat kumpulan url berita yang ada pada halaman web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(nasional: int, internasional: int, halaman: int=None):\n",
    "\turl = [\"https://www.cnnindonesia.com/nasional/indeks/3/\", \"https://www.cnnindonesia.com/internasional/indeks/6/\"]\n",
    "\t# url = [\"https://www.cnnindonesia.com/indeks/2/\"]\n",
    "\tnews = []\n",
    "\n",
    "\tnasional_count = 0\n",
    "\tinternasional_count = 0\n",
    "\n",
    "\twhile url:\n",
    "\t\turl_now = url.pop()\n",
    "\t\tcount = 1 # - n\n",
    "\n",
    "\t\tif halaman is None:\n",
    "\t\t\thalaman = int(get_html(url_now).find(\"div\", class_=\"flex gap-5 my-8 items-center justify-center undefined\").find_all(\"a\")[-2].text)\n",
    "\t\t\t# print(f\"Total page: {halaman}\")\n",
    "\n",
    "\t\tfor _ in range(halaman):\n",
    "\t\t\tpage = f\"{url_now}{count}\"   \n",
    "\t\t\tsoup = get_html(page)\n",
    "\t\t\tnews_list = get_news(soup)\n",
    "\t\t\t\n",
    "\t\t\tfor item in tqdm(news_list, desc=f\"Processing page {count}\"):\n",
    "\t\t\t\t# news_url = item.find('a')['href']\n",
    "\t\t\t\tnews_url = \"https://www.cnnindonesia.com/internasional/20240913095807-113-1144056/jepang-kerahkan-jet-tempur-saat-pesawat-rusia-mengitari-negeri-sakura\"\n",
    "\t\t\t\t# print(news_url)\n",
    "\t\t\t\tresult = scrape_news(get_html(news_url))\n",
    "\n",
    "\t\t\t\tif nasional_count >= nasional and internasional_count >= internasional:\n",
    "\t\t\t\t\treturn news\n",
    "\n",
    "\t\t\t\tif result['kategori'] == 'Nasional' and nasional_count < nasional:\n",
    "\t\t\t\t\tnews.append(result)\n",
    "\t\t\t\t\tnasional_count += 1\n",
    "\t\t\t\telif result['kategori'] == 'Internasional' and internasional_count < internasional:\n",
    "\t\t\t\t\tnews.append(result)\n",
    "\t\t\t\t\tinternasional_count += 1\n",
    "\t\t\t\t\n",
    "\t\t\tcount+=1\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\n",
    "\t\t\tif (nasional_count >= nasional and 'nasional' in url_now) or (internasional_count >= internasional and 'internasional' in url_now):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\treturn news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menyiapkan link/base url web berita yang ingin dicrawling, terdapat beberapa fungsi yang dipanggil yang sudah dibuat sebelumnya untuk mengambil informasi atau berita pada halaman website. Dalam code tersebut terdapat beberapa tahapan seperti fungsi:\n",
    "* get_html\n",
    "* get_news\n",
    "* scrape_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing page 1:  10%|â–ˆ         | 1/10 [00:00<00:05,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "news = main(nasional=0, internasional=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menjalankan program yang sudah dibuat dengan input berapa halaman yang ingin diambil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(news)\n",
    "df\n",
    "df.to_csv(\"data_100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jepang mengerahkan sejumlah jet tempur usai pesawat Rusia untuk pertama kalinya dalam lima tahun terbang mengitari Negeri Sakura.\n",
      "Kementerian Pertahanan Jepang pada Jumat (13/9) menyatakan pesawat Tu-142 milik Rusia terbang dari laut yang memisahkan Jepang-Korea Selatan menuju wilayah Okinawa selatan sejak Kamis (12/9) pagi.\n",
      "\n",
      "Pesawat-pesawat Kremlin itu kemudian menuju ke utara di atas Samudra Pasifik dan mengakhiri penerbangannya di Pulau Hokkaido utara.\n",
      "Pesawat-pesawat ini tidak memasuki wilayah udara Jepang. Namun, mereka terbang di atas kawasan yang menjadi sengketa antara Jepang dan Rusia.\n",
      "\"Sebagai tanggapan, kami mengerahkan jet tempur Pasukan Bela Diri Udara (Angkatan Udara) dalam rangka keadaan darurat,\" demikian keterangan Kementerian Pertahanan Jepang, seperti dikutip AFP.\n",
      "\n",
      "Ini merupakan kali pertama dalam lima tahun Rusia memutari Jepang dengan pesawatnya. Terakhir kali pesawat militer Kremlin mengitari Jepang yaitu pada 2019, kala bomber (pesawat pengebom) Rusia memasuki wilayah udara Negeri Sakura.\n",
      "Pengerahan pesawat ini terjadi setelah Rusia dan China melakukan latihan militer bersama di Laut Jepang awal pekan ini.\n",
      "Latihan itu diklaim Presiden Rusia Vladimir Putin sebagai latihan angkatan laut yang terbesar dalam tiga dekade.\n",
      "\n",
      "Rusia dan China telah meningkatkan kerja sama militer dalam beberapa tahun terakhir. Keduanya kompak menentang dominasi AS atas urusan-urusan global.\n",
      "Hubungan Jepang dan Rusia sendiri memburuk sejak invasi Rusia ke Ukraina pecah. Jepang dan Rusia sejak dulu sama-sama memperebutkan wilayah Kepulauan Kuril, yang dikenal di Jepang sebagai Wilayah Utara.\n",
      "Uni Soviet merebut wilayah itu pada hari-hari terakhir Perang Dunia II, dan terus mempertahankan kehadiran militer di sana sejak itu.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df['isi'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\LAB\n",
      "[nltk_data]     SISTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\LAB\n",
      "[nltk_data]     SISTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Library untuk data manipulation\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Library untuk text preprocessing\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Library untuk text vectorization/TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\ttext = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', text) # Menghapus https* and www*\n",
    "\ttext = re.sub(r'@[^\\s]+', ' ', text) # Menghapus username\n",
    "\ttext = re.sub(r'[\\s]+', ' ', text) # Menghapus tambahan spasi\n",
    "\ttext = re.sub(r'#([^\\s]+)', ' ', text) # Menghapus hashtags\n",
    "\ttext = re.sub(r'rt', ' ', text) # Menghapus retweet\n",
    "\ttext = text.translate(str.maketrans(\"\",\"\",string.punctuation)) # Menghapus tanda baca\n",
    "\ttext = re.sub(r'\\d', ' ', text) # Menghapus angka\n",
    "\ttext = text.lower()\n",
    "\ttext = text.encode('ascii','ignore').decode('utf-8') #Menghapus ASCII dan unicode\n",
    "\ttext = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "\ttext = text.replace('\\n','') #Menghapus baris baru\n",
    "\ttext = text.strip()\n",
    "\treturn text\n",
    "def stemming_indo(text):\n",
    "\tfactory = StemmerFactory()\n",
    "\tstemmer = factory.create_stemmer()\n",
    "\ttext = ' '.join(stemmer.stem(word) for word in text)\n",
    "\treturn text\n",
    "def clean_stopword(tokens):\n",
    "\tlistStopword =  set(stopwords.words('indonesian'))\n",
    "\tremoved = []\n",
    "\tfor t in tokens:\n",
    "\t\tif t not in listStopword:\n",
    "\t\t\tremoved.append(t)\n",
    "\treturn removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.53it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(content):\n",
    "\tresult = []\n",
    "\tfor text in tqdm(content):\n",
    "\t\tcleaned_text = clean_text(text)\n",
    "\t\ttokens = nltk.tokenize.word_tokenize(cleaned_text)\n",
    "\t\tcleaned_stopword = clean_stopword(tokens)\n",
    "\t\tstemmed_text = stemming_indo(cleaned_stopword)\n",
    "\t\tresult.append(stemmed_text)\n",
    "\treturn result\n",
    "\n",
    "df['cleaned_text'] = preprocess_text(df['isi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vsm(data, kategori):\n",
    "\ttfidf = TfidfVectorizer()\n",
    "\ttfidf_matrix = tfidf.fit_transform(data)\n",
    "\tfeature_names = tfidf.get_feature_names_out()\n",
    "\t\n",
    "\tdf_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\tdf_tfidf.insert(0, 'Kategori Berita', kategori.reset_index(drop=True))\n",
    "\n",
    "\treturn tfidf, df_tfidf\n",
    "\n",
    "# tfidf, df_tfidf = tfidf_vsm(df['cleaned_text'], df['kategori'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tugas3/lr_model.pkl', 'rb') as file:  \n",
    "    # Call load method to deserialze \n",
    "    model = pickle.load(file)\n",
    "with open('tugas2/tfidf_vsm.pkl', 'rb') as file:  \n",
    "    # Call load method to deserialze \n",
    "    tfidf = pickle.load(file)\n",
    "\n",
    "tfidf_matrix = tfidf.transform(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAB SISTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain the logistic regression model with the updated feature matrix\n",
    "model.predict(tfidf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
